# ğŸ¤– ALQ-Agent Module

> **Adaptive Learning Question Agent** - Há»‡ thá»‘ng AI Agent tá»± Ä‘á»™ng sinh cÃ¢u há»i thÃ­ch á»©ng theo ká»¹ nÄƒng yáº¿u cá»§a há»c sinh tiá»ƒu há»c Viá»‡t Nam

## ğŸ“‹ Giá»›i thiá»‡u

Module **agent** lÃ  trÃ¡i tim cá»§a há»‡ thá»‘ng Mini Adaptive Learning, Ä‘áº£m nháº­n vai trÃ²:

- ğŸ¯ **Sinh cÃ¢u há»i thÃ´ng minh**: Tá»± Ä‘á»™ng táº¡o cÃ¢u há»i phÃ¹ há»£p vá»›i trÃ¬nh Ä‘á»™ vÃ  ká»¹ nÄƒng yáº¿u cá»§a há»c sinh
- ğŸ“š **RAG-powered**: Káº¿t há»£p kiáº¿n thá»©c tá»« SGV (SÃ¡ch GiÃ¡o ViÃªn) vÃ  SGK (SÃ¡ch GiÃ¡o Khoa) qua vector search
- âœ… **Validation tá»± Ä‘á»™ng**: Kiá»ƒm Ä‘á»‹nh cÃ¢u há»i vá» máº·t logic, toÃ¡n há»c, ngÃ´n ngá»¯ vá»›i 4 bÆ°á»›c validation
- ğŸ”„ **Multi-LLM Hub**: Há»— trá»£ nhiá»u LLM provider vá»›i cÆ¡ cháº¿ fallback, retry vÃ  circuit breaker
- ğŸ“ **Chuáº©n chÆ°Æ¡ng trÃ¬nh VN**: TuÃ¢n thá»§ chuáº©n kiáº¿n thá»©c ká»¹ nÄƒng giÃ¡o dá»¥c tiá»ƒu há»c Viá»‡t Nam
- ğŸ“Š **PhÃ¢n bá»• Ä‘á»™ khÃ³ thÃ­ch á»©ng**: Tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh Ä‘á»™ khÃ³ dá»±a trÃªn hiá»‡u suáº¥t há»c sinh

## ğŸ› ï¸ Tech Stack

### Core Dependencies
```
- Python 3.10+
- PyYAML (config management)
- Requests (HTTP client for LLM APIs)
```

### LLM Providers
- **Ollama** (local LLM, priority 1) - Gemma2:9b
- **Google Gemini** (cloud LLM, priority 2) - Gemini 2.0 Flash Lite
- Extensible cho OpenAI, Anthropic, etc.

### Vector Database
- **Milvus** (via `database/milvus/`)
- Vietnamese embedding model (via `database/embeddings/`)

### Related Services
- MongoDB (student profiles, question history)
- Quiz API (REST endpoints)

## ğŸ“¦ CÃ i Ä‘áº·t

### 1. Install Dependencies

```bash
# From project root
pip install -r requirements.txt
```

### 2. Setup Vector Database

```bash
# Start Milvus (via Docker Compose)
cd volumes/milvus
docker-compose up -d

# Insert data to Milvus
python database/milvus/setup_milvus.py
python database/milvus/insert_data_milvus.py
```

### 3. Setup LLM Providers

**Option A: Ollama (Local)**
```bash
# Install Ollama: https://ollama.ai
# Pull model
ollama pull gemma2:9b

# Verify
curl http://localhost:11434/api/tags
```

**Option B: Google Gemini**
```bash
# Set API key
export GEMINI_API_KEY="your-api-key"
# Windows PowerShell
$env:GEMINI_API_KEY="your-api-key"
```

## âš™ï¸ Cáº¥u hÃ¬nh

### Environment Variables

```bash
# .env file
GEMINI_API_KEY=your-google-api-key-here

# Milvus connection
MILVUS_HOST=localhost
MILVUS_PORT=19530

# MongoDB connection
MONGODB_URI=mongodb://localhost:27017
```

### Agent Configuration

File: `configs/agent.yaml`

```yaml
llm:
  providers:
    - name: gemma2:9b
      type: ollama
      base_url: OLLAMA_IP    # http://localhost:11434
      model: gemma2:9b
      priority: 1            # Lower = higher priority
      timeout_s: 15
    
    - name: gemini
      type: google_gemini
      model: gemini-2.0-flash-lite
      api_key_env: GEMINI_API_KEY
      priority: 2
      timeout_s: 15
  
  retry: 1                   # Retry per provider
  temperature_default: 0.2
  max_tokens: 1024
  circuit_breaker:
    failure_threshold: 3     # Open circuit after N failures
    cooldown_s: 120          # Cooldown period

rag:
  topk_sgv: 5                # Top-K teacher context
  topk_sgk: 20               # Top-K textbook context
  cache_ttl_s: 900           # Cache TTL

question_generation:
  batch_size: 4              # Questions per batch (3-5)
  temperature: 0.3           # Temperature for generation
  max_tokens: 2048           # Max tokens for generation
  retry_on_parse_error: 2    # Retry on JSON parse errors
  enforce_4_answers: true    # Force 4 options for multiple choice
  enable_teacher_summary: true              # Enable teacher context summarization
  teacher_summary_mode: llm_then_rule       # llm_only | rule_only | llm_then_rule
  teacher_summary_max_tokens: 400           # Max tokens for summary
  teacher_summary_max_words: 180            # Max words for summary

images:
  base_url: http://125.212.229.11:8888/    # Base URL for images

validation:
  min_len: 6                 # Min question length (chars)
  max_len: 180               # Max question length (chars)
  banned_words: ["tá»¥c tÄ©u", "báº¡o lá»±c"]
  require_abcd_format: true  # Require ABCD format
  unique_options: true       # Unique answer options
  grade_numeric_range:
    grade1: [0, 100]         # Numeric range for grade 1
  enable_math_check: true    # Verify math calculations
  enable_llm_critique: false # Use LLM for critique
  auto_fix_once: true        # Auto-fix issues once

workflow:
  regen_limit: 2             # Max regeneration attempts
  min_score: 0.0             # Min RAG relevance score
  max_teacher_ctx: 5         # Max teacher context chunks
  max_textbook_ctx: 20       # Max textbook context chunks
```

## ğŸš€ Cháº¡y Module

### Development Mode

```python
# Python API
from agent.workflow import AgentWorkflow

workflow = AgentWorkflow()

result = workflow.run(
    profile_student={
        "username": "student1",
        "accuracy": 0.5,
        "skill_id": "S5",
        "answered": 0.7,      # Tá»· lá»‡ tráº£ lá»i
        "skipped": 0.2,       # Tá»· lá»‡ bá» qua
        "avg_response_time": 45  # Thá»i gian tráº£ lá»i TB (giÃ¢y)
    },
    constraints={
        "grade": 1,
        "skill": "S5",
        "skill_name": "Máº¥y vÃ  máº¥y",
        "num_questions": 6
    }
)

print(result["questions"])
```

### Via REST API

```bash
# Start Quiz API server
cd backend/quiz_api
uvicorn main:app --host 0.0.0.0 --port 8000

# Generate questions
curl -X POST http://localhost:8000/agent/questions/generate \
  -H "Content-Type: application/json" \
  -d '{
    "student_id": "student1",
    "grade": 1,
    "skill": "S5",
    "skill_name": "Máº¥y vÃ  máº¥y",
    "num_questions": 6
  }'
```

### Run Tests

```bash
# Unit tests
pytest tests/test_question_generation_tool.py
pytest tests/test_rag_tool.py
pytest tests/test_validation_tool.py

# Integration test
pytest tests/test_workflow.py

# Test with coverage
pytest --cov=agent tests/
```

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c

```
agent/
â”œâ”€â”€ __init__.py                    # Package initializer
â”‚
â”œâ”€â”€ llm/                           # ğŸ§  LLM Hub & Providers
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ hub.py                     # Multi-provider orchestration
â”‚   â”œâ”€â”€ provider_base.py           # Abstract base class
â”‚   â”œâ”€â”€ provider_ollama.py         # Ollama integration
â”‚   â””â”€â”€ provider_gemini.py         # Google Gemini integration
â”‚
â”œâ”€â”€ models/                        # ğŸ“Š Data Models
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ types.py                   # TypedDict, Pydantic schemas
â”‚
â”œâ”€â”€ prompts/                       # ğŸ“ Prompt Templates
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ generation_prompts.py      # Question generation prompts
â”‚   â”œâ”€â”€ validation_prompts.py      # Validation critique prompts
â”‚   â””â”€â”€ refine_prompts.py          # Question refinement prompts
â”‚
â”œâ”€â”€ tools/                         # ğŸ”§ Core Tools
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ _json_parser.py            # LLM output parser
â”‚   â”œâ”€â”€ rag_tool.py                # Vector search & context retrieval
â”‚   â”œâ”€â”€ question_generation_tool.py # Question generator
â”‚   â””â”€â”€ validation_tool.py         # Question validator
â”‚
â””â”€â”€ workflow/                      # ğŸ”„ Orchestration
    â”œâ”€â”€ __init__.py
    â””â”€â”€ agent_workflow.py          # Main workflow engine
```

### Chi tiáº¿t cÃ¡c thÃ nh pháº§n

#### ğŸ§  LLM Hub (`llm/`)
- **hub.py**: Priority-based LLM orchestration vá»›i retry, circuit breaker, soft-gate
- **provider_base.py**: Abstract base class cho LLM providers
- **provider_ollama.py**: Ollama integration vá»›i healthcheck
- **provider_gemini.py**: Google Gemini integration

#### ğŸ“Š Models (`models/`)
- **types.py**: TypedDict, Pydantic schemas cho data models

#### ğŸ“ Prompts (`prompts/`)
- **generation_prompts.py**: Question generation prompts vá»›i 4-step validation
- **validation_prompts.py**: Validation critique prompts
- **refine_prompts.py**: Question refinement prompts

#### ğŸ”§ Tools (`tools/`)
- **rag_tool.py**: Truy váº¥n Milvus, rerank, merge context tá»« SGV + SGK, caching
- **question_generation_tool.py**: Sinh cÃ¢u há»i tá»« context + student profile, teacher context summarization
- **validation_tool.py**: Rule-based + math checks + optional LLM critique, auto-fix
- **_json_parser.py**: LLM output parser vá»›i error handling

#### ğŸ”„ Workflow (`workflow/`)
- **agent_workflow.py**: Äiá»u phá»‘i toÃ n bá»™ pipeline: RAG â†’ Generate â†’ Validate â†’ Refine

## ğŸ”Œ API/Usage

### AgentWorkflow

```python
from agent.workflow import AgentWorkflow

workflow = AgentWorkflow()

# Generate questions
result = workflow.run(
    profile_student={
        "username": "student1",
        "accuracy": 0.5,         # Current accuracy (0-1)
        "skill_id": "S5",        # Weak skill
        "answered": 0.7,         # Tá»· lá»‡ tráº£ lá»i (0-1)
        "skipped": 0.2,          # Tá»· lá»‡ bá» qua (0-1)
        "avg_response_time": 45  # Thá»i gian tráº£ lá»i TB (giÃ¢y)
    },
    constraints={
        "grade": 1,              # Grade level
        "skill": "S5",           # Skill code
        "skill_name": "Máº¥y vÃ  máº¥y",  # Skill name
        "num_questions": 6       # Number of questions
    }
)

# Output structure
{
    "questions": [
        {
            "question_id": "q1_abc123",
            "question_text": "3 + 2 = ?",
            "question_type": "multiple_choice",  # true_false | multiple_choice | fill_blank
            "difficulty": "easy",                # easy | medium | hard
            "answers": [
                {"text": "3", "correct": false},
                {"text": "5", "correct": true},
                {"text": "4", "correct": false},
                {"text": "6", "correct": false}
            ],
            "explanation": "3 + 2 = 5",
            "provenance": {
                "teacher_ids": ["vec_1", "vec_2"],
                "textbook_ids": ["vec_10"],
                "provider": "gemma2:9b",
                "timestamp": "2025-10-22T10:30:00Z"
            }
        },
        # ... more questions
    ],
    "metadata": {
        "attempts": 1,
        "timings": {
            "gen_attempt_1": 1200,  # ms
            "val_attempt_1": 300
        },
        "validation": {
            "status": "approved",
            "issues": []
        },
        "provider_used": "llm_hub"
    }
}
```

### RAGTool (Standalone)

```python
from agent.tools import RAGTool

rag = RAGTool()

result = rag.query(
    grade=1,
    skill="S5",
    skill_name="Máº¥y vÃ  máº¥y",
    topk_sgv=5,   # Teacher context
    topk_sgk=20   # Textbook context
)

print(result["teacher_context"])  # SGV chunks
print(result["textbook_context"]) # SGK examples
```

### QuestionGenerationTool (Standalone)

```python
from agent.tools import QuestionGenerationTool
from agent.llm.hub import LLMHub

hub = LLMHub()
generator = QuestionGenerationTool(hub)

result = generator.generate(
    teacher_context=[...],
    textbook_context=[...],
    profile_student={...},
    constraints={...}
)

print(result["questions"])
```

### ValidationTool (Standalone)

```python
from agent.tools import ValidationTool

validator = ValidationTool()

report = validator.validate(
    questions=[...],
    skill="S5",
    teacher_context=[...],
    textbook_context=[...],
    grade=1
)

# report structure
{
    "status": "approved" | "revise",
    "issues": [
        {
            "code": "DUP_OPTION",
            "message": "question q1: duplicated answer options"
        }
    ],
    "suggested_fixes": [...],      # From LLM critique (if enabled)
    "applied_fixes": [...],        # Auto-applied fixes
    "confidence": 0.92,
    "validated_questions": [...],  # Questions after auto-fix
    "debug_flags": {
        "enable_llm_critique": false,
        "hub_attached": false,
        "critique_branch_executed": false
    }
}
```

## ğŸ¯ TÃ­nh nÄƒng chÃ­nh

### 1. Adaptive Difficulty Distribution (PhÃ¢n bá»• Ä‘á»™ khÃ³ thÃ­ch á»©ng)

Há»‡ thá»‘ng tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh Ä‘á»™ khÃ³ cÃ¢u há»i dá»±a trÃªn hiá»‡u suáº¥t há»c sinh:

- **Accuracy < 50%**: 60% EASY, 30% MEDIUM, 10% HARD
- **Accuracy 50-70%**: 30% EASY, 50% MEDIUM, 20% HARD  
- **Accuracy > 70%**: 20% EASY, 30% MEDIUM, 50% HARD
- **Skipped > 30%**: Táº¡o cÃ¢u há»i rÃµ rÃ ng hÆ¡n
- **Avg time > 60s**: Táº¡o cÃ¢u há»i ngáº¯n gá»n hÆ¡n

### 2. Teacher Context Summarization (TÃ³m táº¯t ngá»¯ cáº£nh giÃ¡o viÃªn)

Giáº£m noise vÃ  tá»‘i Æ°u prompt khi context quÃ¡ dÃ i:

- **Mode**: `llm_only` | `rule_only` | `llm_then_rule`
- **Max tokens**: 400 tokens
- **Max words**: 180 tá»«
- Fallback tá»± Ä‘á»™ng náº¿u LLM summarization tháº¥t báº¡i

### 3. Multi Question Types (Nhiá»u loáº¡i cÃ¢u há»i)

- **true_false**: 2 Ä‘Ã¡p Ã¡n (ÄÃºng/Sai)
- **multiple_choice**: 4 Ä‘Ã¡p Ã¡n (1 Ä‘Ãºng, 3 sai)
- **fill_blank**: 4 Ä‘Ã¡p Ã¡n (Ä‘iá»n vÃ o chá»— trá»‘ng)

### 4. 4-Step Validation Process (Quy trÃ¬nh 4 bÆ°á»›c kiá»ƒm Ä‘á»‹nh)

1. **TÃ­nh toÃ¡n Ä‘Ã¡p Ã¡n Ä‘Ãºng**: Giáº£i bÃ i toÃ¡n thá»§ cÃ´ng
2. **Táº¡o cÃ¡c Ä‘Ã¡p Ã¡n sai**: Há»£p lÃ½ (sai sá»‘ Â±1, Â±2)
3. **XÃ¡c nháº­n correct flag**: CHá»ˆ 1 Ä‘Ã¡p Ã¡n cÃ³ `"correct": true`
4. **Double check**: Äáº¿m sá»‘ Ä‘Ã¡p Ã¡n Ä‘Ãºng/sai

### 5. Automatic Question Fixing (Tá»± Ä‘á»™ng sá»­a lá»—i)

- Sá»­a duplicate options
- Normalize text formatting
- Fix missing required fields
- Adjust answer count

### 6. Circuit Breaker & Retry Logic (Báº£o vá»‡ vÃ  thá»­ láº¡i)

- **Failure threshold**: 3 lá»—i â†’ má»Ÿ circuit
- **Cooldown**: 120s trÆ°á»›c khi thá»­ láº¡i
- **Retry per provider**: 1 láº§n
- **Temperature decay**: Giáº£m 0.1 má»—i retry

## ğŸ”— Káº¿t ná»‘i Module KhÃ¡c

### Dependencies

```python
# Internal dependencies
from database.milvus.milvus_client import MilvusClient
from database.embeddings.local_embedder import embed_text_quick
from database.mongodb.mongodb_client import MongoDBClient

# External APIs
# - Quiz API: POST /agent/questions/generate
# - SAINT Analysis: Student knowledge state
```

### Integration Points

1. **Database Layer**
   - `database/milvus/`: Vector search cho SGV/SGK
   - `database/mongodb/`: Student profiles, question history

2. **Backend Services**
   - `backend/quiz_api/`: REST API wrapper
   - `backend/saint_analysis/`: Student knowledge modeling

3. **Frontend**
   - `frontend/quiz-app/`: UI hiá»ƒn thá»‹ cÃ¢u há»i

### Workflow Integration

```
User (Frontend)
    â†“
Quiz API (backend/quiz_api/)
    â†“
AgentWorkflow (agent/)
    â†“
â”œâ”€â†’ RAGTool â†’ Milvus (database/milvus/)
â”œâ”€â†’ LLMHub â†’ Ollama/Gemini (agent/llm/)
â””â”€â†’ ValidationTool â†’ [rule checks + math checks]
    â†“
MongoDB (database/mongodb/) [save history]
    â†“
Response â†’ Frontend
```

## ğŸ› Troubleshooting

### 1. LLM Connection Issues

**Error**: `LLM_FALLBACK_EXHAUSTED`

```bash
# Check Ollama
curl http://localhost:11434/api/tags

# Check Gemini API key
echo $GEMINI_API_KEY

# Check provider config in configs/agent.yaml
# Verify priority, timeout_s, base_url
```

### 2. Empty Context from RAG

**Error**: `teacher_context` or `textbook_context` is empty

```bash
# Verify Milvus collections
python -c "
from database.milvus.milvus_client import MilvusClient
client = MilvusClient()
print(client.get_collection_stats('sgv_collection'))
print(client.get_collection_stats('baitap_collection'))
"

# Check if data was inserted
python database/mongodb/insert_data_mongodb.py
python database/milvus/insert_data_milvus.py
```

### 3. Validation Always Fails

**Issue**: Questions keep getting rejected

```yaml
# In configs/agent.yaml, adjust validation:
validation:
  enable_math_check: false   # Disable if too strict
  enable_llm_critique: false
  auto_fix_once: true
  min_len: 5                 # Reduce min length
  max_len: 200               # Increase max length

workflow:
  regen_limit: 3             # Increase retry limit
```

### 4. Slow Generation

**Issue**: Generation takes > 10 seconds

```yaml
# Optimize config:
rag:
  topk_sgv: 3                # Reduce context size
  topk_sgk: 10

question_generation:
  batch_size: 3              # Generate fewer questions per batch
  max_tokens: 1024           # Reduce token limit
  enable_teacher_summary: true
  teacher_summary_mode: rule_only  # Faster than LLM

llm:
  timeout_s: 10              # Reduce timeout
```

### 5. Circuit Breaker Triggered

**Error**: Provider blocked by circuit breaker

```bash
# Wait for cooldown_s (default 120s)
# Or restart application to reset

# Adjust thresholds in configs/agent.yaml:
llm:
  circuit_breaker:
    failure_threshold: 5  # Increase tolerance
    cooldown_s: 60        # Reduce cooldown
```

### 6. JSON Parse Errors

**Error**: `ParseError: Could not extract valid JSON`

```yaml
# In configs/agent.yaml:
question_generation:
  retry_on_parse_error: 3  # Increase retry
  temperature: 0.2         # Lower temperature for more deterministic output
```

### 7. Math Validation Failures

**Issue**: Correct answers marked as wrong

```yaml
# Check validation logic in agent/tools/validation_tool.py
# Adjust grade_numeric_range:
validation:
  grade_numeric_range:
    grade1: [0, 100]  # Adjust range for grade
  enable_math_check: true
```

### 8. Teacher Context Too Long

**Issue**: Context exceeds token limit

```yaml
# Enable teacher context summarization:
question_generation:
  enable_teacher_summary: true
  teacher_summary_mode: llm_then_rule  # or rule_only for speed
  teacher_summary_max_tokens: 400
  teacher_summary_max_words: 180

# Or reduce context size:
rag:
  topk_sgv: 3  # Reduce from 5
```

### 9. Wrong Answer Marked as Correct

**Issue**: "correct" flag doesn't match calculation result

```python
# This is prevented by 4-step validation in prompts
# Check generation_prompts.py SYSTEM_PROMPT for validation steps
# If still happening, increase temperature for more careful generation:
question_generation:
  temperature: 0.2  # Lower temperature = more deterministic
  retry_on_parse_error: 3  # More retries
```

---

## ğŸ“š TÃ i liá»‡u tham kháº£o

- [Database README](../database/README.md) - Database setup guide
- [Quiz API README](../backend/quiz_api/README.md) - REST API docs
- [Configs README](../configs/README.md) - Configuration guide

---

## ğŸ¯ Roadmap

- [x] 4-step validation process
- [x] Adaptive difficulty distribution
- [x] Teacher context summarization
- [x] Multi question types (true_false, multiple_choice, fill_blank)
- [x] Circuit breaker & retry logic
- [x] Automatic question fixing
- [ ] Refine tool vá»›i feedback giÃ¡o viÃªn
- [ ] A/B testing framework cho prompts
- [ ] Multi-language support (English, etc.)
- [ ] Real-time streaming generation
- [ ] Question difficulty prediction model

---

**Maintainer**: Mini Adaptive Learning Team  
**Last Updated**: October 22, 2025
